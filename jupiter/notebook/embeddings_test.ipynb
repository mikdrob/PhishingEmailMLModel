{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, roc_curve, \\\n",
    "    precision_recall_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the path to the input file\n",
    "INPUT_FILE_PATH = \"../dataset/input/embeddings.csv\"\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(INPUT_FILE_PATH)\n",
    "\n",
    "# Convert the string embeddings to arrays\n",
    "df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)\n",
    "embedding_values = list(df.embedding.values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embedding_values, df.is_phishing, test_size=0.3, random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train a random forest classifier on the training set\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "preds = clf.predict(X_test)\n",
    "probas = clf.predict_proba(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the performance of the classifier using various metrics\n",
    "report = classification_report(y_test, preds, digits=4)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds)\n",
    "roc_auc = roc_auc_score(y_test, probas[:, 1])\n",
    "\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Accuracy: {:.5f}\".format(accuracy))\n",
    "print(\"F1 Score: {:.5f}\".format(f1))\n",
    "print(\"ROC AUC Score: {:.5f}\".format(roc_auc))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute precision, recall and thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, probas[:, 1])\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.plot(recall, precision, color='blue', label='Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute the false positive rate and true positive rate for the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probas[:, 1])\n",
    "roc_auc = roc_auc_score(y_test, probas[:, 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, linestyle='-', label='ROC curve (AUC = {:.4f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random guess')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute and plot confusion matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Not phishing', 'Phishing'], rotation=45)\n",
    "plt.yticks(tick_marks, ['Not phishing', 'Phishing'])\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
